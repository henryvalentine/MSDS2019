---
title: "DATA 607 Project 2"
author: "Henry Otuadinma"
date: "24 February 2019"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)

library(xml2)
library(rvest)
library(stringr)
library(DT)
library(dplyr)
library(tidyr)
library(ggplot2)
```


#### Data set 1: Top 5 Best Seller books on Amazon.

This is a dataset that will contain top best selling and highly rated books across 5 different genres (History, Law, Literature & Fiction, Science and Math, and Religion) scrapped from Amazon. I want to purchase the cheapest 5 (one from each genre) among them.

#### Overview

I will produced a .csv file containing 250 observations and 8 variables describing up to date record of 50 top selling books across 5 top genres each scrapped from Amazon.

The data will contain descriptive observations in a fairly clean form because specific information were looked for on the web pages. Each book will have eight variables depicting its title, author, genre, rating, price, type (kindle edition, paperback, etc), etc. 

The data will then be worked on using appropriate functions like `group_by`, `filter`, `arrange`, `select`, and other relevant functions from the `dplyr` and `tidyr` packages to see how the books compare with each others in terms of their genres, types, prices, and the most rated such that at the end, I will be able to to know the cheapest 5 (one from each of the 5 genres) from among them.

#### Data Scrapping

This involves scrapping the the pages of each genre for the required information.

```{r}

religionRaw <- html_nodes(read_html('https://www.amazon.com/Best-Sellers-Books-Religion-Spirituality/zgbs/books/22/ref=zg_bs_nav_b_1_b', encoding = "en_US.UTF-8"), 'ol#zg-ordered-list')

sciMathRaw <- html_nodes(read_html('https://www.amazon.com/Best-Sellers-Books-Science-Math/zgbs/books/75/ref=zg_bs_nav_b_1_b', encoding = "en_US.UTF-8"), 'ol#zg-ordered-list')

litFictionRaw <- html_nodes(read_html('https://www.amazon.com/Best-Sellers-Books-Literature-Fiction/zgbs/books/17/ref=zg_bs_nav_b_1_b', encoding = "en_US.UTF-8"), 'ol#zg-ordered-list')

historyRaw <- html_nodes(read_html('https://www.amazon.com/Best-Sellers-Books-History/zgbs/books/9/ref=zg_bs_nav_b_1_b', encoding = "en_US.UTF-8"), 'ol#zg-ordered-list')

lawRaw <- html_nodes(read_html('https://www.amazon.com/Best-Sellers-Books-Law/zgbs/books/10777/ref=zg_bs_nav_b_1_b', encoding = "en_US.UTF-8"), 'ol#zg-ordered-list')

```

Some of the book titles contain `:` which implies a long decriptive part of the tile comes after the colon. These are therefore
shortened at the `:`

```{r}

processNodes <- function(rawNode, genre)
{
    imgs <- vector()
    genres <- vector()
    titles <- vector()
    authors <- vector()
    ratings <- vector()
    numRaters <- vector()
    types <- vector()
    prices <- vector()
    
    itemsRaw <-html_nodes(rawNode, 'span.zg-item')
  
    for(i in 1: length(itemsRaw))
    {
        x <- itemsRaw[i]
        xImg <- str_replace_all(str_extract(x, '(?<=src=)"(.+?)"'), '\"', '')
        xTitle <- str_trim(str_replace_all(html_text(html_node(x, '.a-link-normal > div')), "[\r\n]" , ""), side='both') 
        
        if(str_detect(xTitle, ':') == TRUE)
        {
          xTitle <- str_split(xTitle, ':')[[1]][1] # Shorten the title by splitting at the colon (:)
        }
        
        xAuthor <- str_trim(html_text(html_node(x, '.a-link-child')), side='both')
        
        if(is.na(xAuthor) || is.null(xAuthor))
        {
          xAuthor <- str_trim(html_text(html_node(x, 'span.a-color-base')), side='both')
        }
        
        xRating <- str_trim(str_extract(html_node(x, '.a-icon-alt'), '\\d+(\\.\\d+)'), side='both')
        
        xNumRaters <- str_replace(str_trim(html_text(html_node(x, 'div.a-icon-row > a.a-size-small')), side='both'), ',', '')
        
        if(is.na(xNumRaters) || is.null(xNumRaters))
        {
          xNumRaters <- str_replace(str_trim(html_text(html_node(x, 
                        'div.a-spacing-none > a-link-normal')), side='both'), ',', '')
        }
       
        xType <- str_trim(html_text(html_node(x, 'div.a-size-small > span.a-color-secondary')), side='both')
        
        p <- str_extract(html_node(x, 'span.p13n-sc-price'), '\\d+(\\.\\d+)')
        
        xPrice <-  round(as.numeric(p), 2)
        
        if(is.na(xPrice) || is.null(xPrice))
        {
          xPrice <- str_extract(str_trim(html_text(html_node(x, 
                        'span.a-color-price')), side='both'), '[[:alpha:] ]{2,}')
        }
        
        imgs <- c(imgs, xImg)
        genres <- c(genres, genre)
        titles <- c(titles, xTitle)
        authors <- c(authors, xAuthor)
        ratings <- c(ratings, xRating)
        numRaters <- c(numRaters, xNumRaters)
        types <- c(types, xType)
        prices <- c(prices, xPrice)
    }
    
    return(data.frame(title = titles, author = authors, genre = genres, rating = ratings, img = imgs,
                      numRater = numRaters, type = types, price = prices))
}

```


##### This Returns 5 different dataframes of 50 observations and 8 variables
 

```{r, warning=FALSE, message=FALSE}

religion <- processNodes(religionRaw, 'Religion')
science <- processNodes(sciMathRaw, 'Science & Math')
fiction <- processNodes(litFictionRaw, 'Literature & Fiction')
history <- processNodes(historyRaw, 'History')
law <- processNodes(lawRaw, 'Law')

```

##### Combine all datframes into one and check the dimensions of the resultant dataframe

```{r}
allBooks <- rbind(history, religion, science, fiction, law)
dim(allBooks)
```

##### Exporting the data to a .csv file

```{r}

write.csv(allBooks, "topBooks", row.names=FALSE)

```

##### We have 250 books and 8 variables

Preview the data

```{r}

mixedBooks <- subset(allBooks, select=c(title, author, genre, rating, numRater, type, price)) #Remove the book image url

datatable(head(mixedBooks, 10), colnames= c("Title", "Author", "Genre", "Rating", "Number of reviewers", "Type", "Price($)"), class = 'cell-border stripe', options = list(
  initComplete = JS(
    "function(settings, json) {",
    "$(this.api().table().header()).css({'background-color': '#1f77b4', 'color': '#fff', 'text-align': 'center !important'});",
    "$(this.api().table().body()).css({'color': '#000', 'text-align': 'center !important'});",
    "}")
))

```

##### Let's filter out the the `Audible Audiobook` and `Kindle Edition` types since the interest is only on printed types, and convert the `rating`, `numRater (Number of Reviewers)`, and `price` variables from `factor` type to `numeric` type

```{r}

books <- mixedBooks %>% filter(!type %in% "Kindle Edition" & !type %in% "Audible Audiobook") 

books <- books%>% mutate(numRater = as.numeric(as.character(numRater))) %>% mutate(price = as.numeric(as.character(price)))%>%mutate(rating = as.numeric(as.character(rating)))

datatable(head(books, 10), colnames= c("Title", "Author", "Genre", "Rating", "Number of reviewers", "Type", "Price($)"), class = 'cell-border stripe', options = list(
  initComplete = JS(
    "function(settings, json) {",
    "$(this.api().table().header()).css({'background-color': '#1f77b4', 'color': '#fff', 'text-align': 'center !important'});",
    "$(this.api().table().body()).css({'color': '#000', 'text-align': 'center !important'});",
    "}")
))

```


##### Let's see the book with the highest rating and the one with highest number of reviewers

```{r}

 books[which.max(books$numRater),]
```

```{r}

books[which.max(books$rating),]

```

##### This then shows that the book with the highest rating or with a 5-star rating is not the most bought or reviewed. It then makes sense to lay emphasis more on the ones with the highest number of reviewers and still with appreciable ratings say between 4.0 and 5.0

##### Let's see how the genres compare to each other. We will `group_by` the genre and add up the number of reviewers so as to know the most reviewed genre

```{r}

books2 <- books%>% group_by(genre) %>% summarize(reviewers = sum(na.omit(numRater)))%>%select(genre, reviewers)
books2
```

```{r}

ggplot(books2, aes(x=genre, y=reviewers, fill=genre))+
  scale_fill_manual(values = c("#1f77b4", "#26868d", '#D32F2F', '#7B1FA2', '#3F51B5') ) + 
  labs(title="Genre Vs Reviewers", x ="Genre", y="Reviewers") + 
  geom_bar(stat="identity", width=.7, show.legend = FALSE)

```

##### It appears Religion is the most reviewed and consequently most purchased genre among the 5 selected genres

Let's select the 5 most reviewed books in each of the genres

```{r}
topReviewd <- books %>%group_by(genre) %>%top_n(n = 5, wt = numRater)
head(topReviewd, 30)
```

##### With the omission of `NA` values, some genres do not have up to 50 books any more.

##### Finally, let's select the cheapest book from among each of the genres

```{r}
cheapestTopReviewd <- topReviewd%>%top_n(n = 1, wt = -price)
cheapestTopReviewd
```

```{r}

datatable(cheapestTopReviewd, colnames= c("Title", "Author", "Genre", "Rating", "Number of reviewers", "Type", "Price($)"), class = 'cell-border stripe', options = list(
  initComplete = JS(
    "function(settings, json) {",
    "$(this.api().table().header()).css({'background-color': '#1f77b4', 'color': '#fff', 'text-align': 'center !important'});",
    "$(this.api().table().body()).css({'color': '#000', 'text-align': 'center !important'});",
    "}")
))

```

#### Conclusion
##### Finally, I have the 5 cheapest books from among top rated and best selling books from among 5 genres to purchase on Amazon!


#### Data set 2: Social Security Disability Claims - a Dataset provided by Rajwant Mishra 

This is a data set of Social Security Administration Disability benefits Claims from 2008 to 2017 Fiscal Years. Each year has months with each month showing the amount of claims collected through other means (like application in person, etc) and the ones registered via the internet.

Fiscal Year (FY): is the 12-month period from October 1st through September 30th.

We want to see how each monthly claims in each year compare to those of other years and to see if the claims submitted through the internet increased with time or not over the period of 2008 to 2017.

Reading the data from the provided .csv file

```{r}
untidySSD <- read.csv("ssadisability.csv", header = TRUE, stringsAsFactors = FALSE) # Read the .csv file
untidySSD
```

```{r}

dim(untidySSD)

```

The data set has 25 observations and 8 variables.

##### Looking at the data, the monthly totals can be gathered under a single column say `Total`

```{r}

ssda <- untidySSD %>% gather(matches("_Total"), key = 'Totals', value = 'Value')
ssda

```

The columns with `_Internet` can be gathered under say `Internet`

```{r}
ssda1 <- ssda %>% gather(Oct_Internet:Sept_Internet, key = 'Internet', value = 'Volume')
ssda1
```

Remove `FY, _Total, _Internet` from the values in the appropriate columns and adding `20` to the `Fiscal_Year` column,
also remove the commas in the `Value` and `Volume` columns and convert their values to numeric

```{r}

ssda2 <- ssda1
ssda2$Volume <- as.numeric(str_replace_all(ssda2$Volume, ',', ''))
ssda2$Value <- as.numeric(str_replace_all(ssda2$Value, ',', ''))
ssda2$Fiscal_Year <- str_replace_all(ssda2$Fiscal_Year, 'FY', '20')
ssda2$Totals <- str_replace_all(ssda2$Totals, '_Total', '')
ssda2$Internet <- str_replace_all(ssda2$Internet, '_Internet', '')
head(ssda2, 20)
```

Let's see how the total claims for each year compare aginst other years by aggregating all the total monthly claims for each year:
First, let's subset the `Fiscal_Year, Totals, Value, and Volume` columns, renaming `Totals` to `Months` and creating a new column `TotalValue` to be the sum of each month's total figures which is `Value`.  `Volume` is already contained in `Value`. It was made to be a separete a column to point out how much claims were made via internet

```{r}
s3 <- ssda2%>%mutate(TotalValue = Value, Month = Totals)%>%select('Fiscal_Year', 'Month', 'TotalValue')
s3 <- na.omit(s3)
s3
```

The above data can be used to vsualise monthly claims for each year.

Lets further group the data for months of each year together so as to get a total figure for the claims per year

```{r}
s4 <- s3%>%group_by(Fiscal_Year)%>% summarise(Total = sum(TotalValue))
s4
```

A summary of the yearly aggregations

The maximum claims occured in 2012

```{r}
s4[which.max(s4$Total),]
```

While the minimum claims occured in 2017

```{r}
s4[which.min(s4$Total),]
```


```{r}

ggplot(s4, aes(x=Fiscal_Year, y=Total, fill=Fiscal_Year))+
  scale_fill_manual(values = c("#1f77b4", "#26868d", '#440154', '#7B1FA2', '#3F51B5', '#D32F2F', '#00695c', '#303f9f', '#6acb5c', '#2e748e', '#6acb5c','#45bc72' ) ) + 
  labs(title="SSD Claims per year", x ="Fiscal Year", y="Claims") + 
  geom_bar(stat="identity", width=.7, show.legend = FALSE)

```

In overall, this shows that the claims were highest in 2011, and lowest in 2017. Let's see if further visual comparisms can show why the claims were lowest in 2017

Let's see how the monthly claims (via ineternet and other means) over ther years compare to each other

```{r}
ggplot(s3, aes(x = Fiscal_Year, y = TotalValue, fill = Month)) + 
  geom_bar(position="fill", stat = "identity") + 
  labs(title="Monthly SSD Claims per year", x ="Fiscal Year", y="Claims") +
  scale_fill_manual(values = c("#1f77b4", "purple", "#440154", "#45367f", "#26868d", '#440154', '#35618d', '#2e748e', '#2dab81', '#2e748e', '#45367f', '#3d4c89', '#6acb5c', '#2e748e', '#6acb5c','#45bc72') )

```

From this plot, it is obvious that the provided data set has claims for only for months in 2017 hence the low claims rate noted for that year.

##### Now, let's see how the claims over the internet compares yearl and monthly

##### Monthly
```{r}

s5 <- ssda2%>%mutate(Total = Volume, Month = Internet)%>%select('Fiscal_Year', 'Month', 'Total')
s5 <- na.omit(s5)
s5

```


```{r}

s5[which.max(s5$Total),]

```

The highst number of claims filed via the internet occured on August 2014

```{r}
ggplot(s5, aes(x = Fiscal_Year, y = Total, fill = Month)) + 
  geom_bar(position="dodge", stat = "identity") + 
  labs(title="Monthly SSD Claims filed via the Internet per year", x ="Fiscal Year", y="Claims") +
  scale_fill_manual(values = c("#1f77b4", "#26868d", "#440154", "#45367f", "#45bc72", '#6acb5c', '#35618d', '#6acb5c', '#2dab81', '#2e748e', '#45367f', '#3d4c89') ) # + coord_flip()
```

##### The trend was progressive incrementally from 2008

##### Yearly

```{r}
s6 <- s5%>%group_by(Fiscal_Year)%>% summarise(Total = sum(Total))
s6
```

```{r}

ggplot(s6, aes(x=Fiscal_Year, y=Total, fill=Fiscal_Year))+
  scale_fill_manual(values = c("#1f77b4", "#26868d", '#440154', '#7B1FA2', '#3F51B5', '#D32F2F', '#00695c', '#303f9f', '#2ca02c', '#ff7f0e' ) ) + 
  labs(title="SSD Claims via the internet per year", x ="Fiscal Year", y="Claims") + 
  geom_bar(stat="identity", width=.7, show.legend = FALSE)

```

```{r}

ggplot(s5, aes(x = Fiscal_Year, y = Total)) + 
     geom_line(color = "#00AFBB", size = 3) + 
  labs(title="Monthly SSD Claims via the Internet per year", x ="Fiscal Year", y="Claims")

```

#### Conclusion 
##### It is evident that the claims filed via the internet generally increased progressively from 2008, achieving maximum peak in 2014 but suprisingly started decreasing progressively from that point.


#### Data set 3: Color and heat absorption - Christopher Ayre

Description: This is a dataset provided from an experiment with his daughter to complete her project for a science fair. For the experiment, they placed thermometers in 5 different color t-shirts and recorded the temperatures at 10 minute intervals for 1 hour as the garments were exposed to heat. The heater was then turned off, and the temperatures measured again at 10 minute intervals as the garments cooled. The attached provided .csv file represents their findings.


##### The experiment question then is to analyze the rate at which different colors absorb and release heat.

Let's read the data

```{r}
untidyHeat <- read.csv("science_proj_data.csv", header = TRUE, stringsAsFactors = FALSE) # Read the .csv file
untidyHeat
```

```{r}

dim(untidyHeat)

```
The dataset has 10 observations and 9 variables.

Let's gather the minutes colums under say `minute` and their values under `temperature`, and then remove `minute.` from the values

```{r}
temp <- untidyHeat %>% gather(minute.0:minute.60, key = 'minute', value = 'temperature')%>%mutate(minute = str_replace_all(minute, 'minute.', ''))
temp
```

Let's how the colors fare during heating and cooling phases

##### For heating phase

```{r}

heating <- subset(temp, phase == 'heating')

```

```{r}

#ggplot(temp, aes(x = minute, y = temperature)) + geom_line()

 ggplot(heating, aes(x = minute, y = temperature, fill = color)) + 
  geom_bar(position="dodge", stat = "identity") + 
  labs(title="Temperature performances of the colors during heating phase", x ="Minute", y="Temperature") +
  scale_fill_manual(values = c("#000000", "#2ca02c", "#440154", "#D32F2F", "#45367f") )

```


##### For cooling phase

```{r}

cooling <- subset(temp, phase == 'cooling')

```

```{r}

#ggplot(temp, aes(x = minute, y = temperature)) + geom_line()

 ggplot(cooling, aes(x = minute, y = temperature, fill = color)) + 
  geom_bar(position="dodge", stat = "identity") + 
  labs(title="Temperature performances of the colors during cooling phase", x ="Minute", y="Temperature") +
  scale_fill_manual(values = c("#000000", "#2ca02c", "#440154", "#D32F2F", "#45367f") )

```

#### Conclusion

##### From all indications, Black colors tend tend to absorb heat the most (very fast) and slowest in releasing (emitting) heat. White colors on the other hand tend to be slowest in absorbing heat and very fast in cooling down (releasing/emiting heat). Red colors appears to retain heat as much and is next to black color in that regards. Other colors fall in-between these ones