---
title: 'DATA607 Project 4: Document Classification'
date: '`r format(Sys.Date(), "%B %d, %Y")`'
output:
  html_document:
    toc: true
    theme: united
    highlight: tango
    toc_float:
      collapsed: false
      smooth_scroll: true
---


```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
library(tidyverse)
library(tidytext)
library(tm)
library(caret)
library(e1071)
library(wordcloud)
library(R.utils)
library(DT)
```

# Project problem

It can be useful to be able to classify new "test" documents using already classified "training" documents.  A common example is using a corpus of labeled spam and ham (non-spam) e-mails to predict whether or not a new document is spam.  

For this project, you can start with a spam/ham dataset, then predict the class of new documents (either withheld from the training dataset or from another source such as your own spam folder).   One example corpus:  https://spamassassin.apache.org/publiccorpus/

Here are two short videos that you may find helpful.

The first video shows how to unzip the provided files.

Solutions:

Packages used

knitr::opts_chunk$set(echo = TRUE)
library(tidyverse)
library(tidytext)
library(tm)
library(caret)
library(e1071)
library(wordcloud)
library(R.utils)
library(DT)

## Download and Extract files
The spam and ham files are downloaded from spamassassin's website:

```{r}

getFiles <- function(files,baseurl)
{
        for (i in 1: length(files))
          {
                fUrl <- str_c(baseurl,files[i])
                download.file(fUrl, destfile = files[i])
                Sys.sleep(1)
        }
}


```

```{r}
targetFiles <- c("20050311_spam_2.tar.bz2","20030228_easy_ham_2.tar.bz2")
```

```{r eval=FALSE}

getFiles(targetFiles,"https://spamassassin.apache.org/old/publiccorpus/")

```

Extract the downloaded files twice. first, as a .bz2 and then as a .tar

```{r eval=FALSE}

for (i in 1: length(targetFiles)) 
{
  bunzip2(targetFiles[i])
  untar(tarfile = str_replace(targetFiles[i], ".bz2", ""))
}


```

##### View all the sub-directories in the current working directory to be sure the files were extracted with their expected contents:

```{r}

list.dirs(path = ".", full.names = TRUE, recursive = TRUE)

```

```{r}
length(list.files("./easy_ham_2"))
```

```{r}
length(list.files("./spam_2"))
```

##### So, as it stands, all the files are now extracted in their separate folders, but I need to be sure that all the files in each of the folders are exactly named as every other on so that I won't try to parse files that are neither a spam nor a ham

```{r}
hamFiles <- list.files("./easy_ham_2")
```

```{r}
spamFiles <- list.files("./spam_2")
```

```{r}
head(hamFiles)
```

```{r}
head(spamFiles)
```

```{r}
(notHamFile <- hamFiles[str_detect(hamFiles,"^\\d+\\.[:alnum:]") == FALSE])
```


```{r}
(notSpamFile <- spamFiles[str_detect(spamFiles,"^\\d+\\.[:alnum:]") == FALSE])
```

##### So it is obvious that there are a lot of files that are neither ham nor spam existing in both folders, therefore, I have to get rid of those files

```{r}
if (file.exists("./easy_ham_2/cmds")) file.remove("./easy_ham_2/cmds")
if (file.exists("./spam_2/cmds")) file.remove("./spam_2/cmds")
```

##### I have to re-fetch the files in the folders again as the `cmds` files have been deleted, and then check the length of the content

```{r}
hamFiles <- list.files("./easy_ham_2")
```

```{r}
spamFiles <- list.files("./spam_2")
```

```{r}
length(spamFiles)
```

## Read all files into a dataframe

```{r}
allEmails <- tibble()
hamSpamFolders <- c("./spam_2", "./easy_ham_2")
fileTypes <-c("spam", "ham")
allFileNames <- c(spamFiles, hamFiles)

```


```{r warning=FALSE}

for (i in 1: length(hamSpamFolders))
  {
        type <- fileTypes[i]
        ff <- tibble(file = dir(hamSpamFolders[i],  full.names = TRUE)) %>% mutate(text = map(file, read_lines)) %>%
                transmute(id = basename(file), type = type, text) %>%
                unnest(text)
                allEmails <- bind_rows(allEmails, ff)
 }
 
```


```{r}

allEmails <- tibble::rowid_to_column(allEmails, "linenumber")

```

```{r}
emailsDf2 <- allEmails
```

```{r}

datatable(head(allEmails), class = 'cell-border stripe', options = list(
  initComplete = JS(
    "function(settings, json) {",
    "$(this.api().table().header()).css({'background-color': '#26868d', 'color': '#fff', 'text-align': 'center !important'});",
    "$(this.api().table().body()).css({'color': '#000', 'text-align': 'center !important'});",
    "}")
))
```

##### Group the dataframe by file type and check how many of each type is in the dataframe

```{r}

allEmails %>% group_by(type) %>% summarize(messages  = n_distinct(id)) %>% ungroup()

```

##### I will remove characters like colons, hyphens, apostrophe, etc and replace them with spaces so that the words they separate will not be combined into one; remove punctuation, convert the corpus to lower case, remove all numbers, remove white spaces, and remove stop words; and finally form the document-term-matrix from the output

```{r}

allEmails$text <- str_replace_all(allEmails$text,"[^[:graph:]]", " ")

```

```{r}
(allCorpus <- Corpus(VectorSource(allEmails$text)))

```

```{r}

stripJoiners <- content_transformer(function(x, pattern) {return (gsub(pattern, " ", x))})

```

```{r warning=FALSE}
cleanMailCorpus <- tm_map(allCorpus, stripJoiners, "-")
cleanMailCorpus <- tm_map(cleanMailCorpus, stripJoiners, ":")
cleanMailCorpus <- tm_map(cleanMailCorpus, stripJoiners, "\\.")
cleanMailCorpus <- tm_map(cleanMailCorpus, stripJoiners, "'")
cleanMailCorpus <- tm_map(cleanMailCorpus, removePunctuation)
cleanMailCorpus <- tm_map(cleanMailCorpus, removeNumbers)
cleanMailCorpus <- tm_map(cleanMailCorpus, tolower)
cleanMailCorpus <- tm_map(cleanMailCorpus, stripWhitespace)
cleanMailCorpus <- tm_map(cleanMailCorpus, removeWords, stopwords("English"))

```


```{r}

mailsDf <- data.frame(text = get("content", cleanMailCorpus))
allEmails$text <- as.character(mailsDf$text)

```

```{r}
head(allEmails)
```

## Corpus and Document Term Matrix

##### Next is to Create a corpus of the files but first striping all non-graphic characters so as to avoid the errors they can cause when `tm_map` is called

```{r}

(allCorpus <- Corpus(VectorSource(allEmails$text )))

```

##### Create document-term matrix to make the data to be in a one-row-per-document format

```{r}

cleanMailsDTM <- DocumentTermMatrix(cleanMailCorpus)

```

## Weighting (tf-idf)
##### This is simply to count how frequently a word occurs in a document. An alternative approach is term frequency inverse document frequency (tf-idf), which is the frequency of a term adjusted for how rarely it is used.

```{r}
allMailsTokens <- allEmails %>%
   unnest_tokens(output = word, input = text) %>%
   # remove numbers
   filter(!str_detect(word, "^[0-9]*$")) %>%
   # remove stop words
   anti_join(stop_words) %>%
   # stem the words
   mutate(word = SnowballC::wordStem(word))

```

```{r}

cleanMailsTokensTfIdf <- allMailsTokens %>%
   count(type, word) %>%
   bind_tf_idf(term = word, document = type, n = n)

```

```{r}
head(cleanMailsTokensTfIdf)
```

## Exploratory analysis
##### Visualisations: There are some very low frequent terms in the clean data. in some cases, rarely occuring terms might be have important descriptive values in the documents they occur.

```{r}

tempDf <- allEmails

mailsDf <- data.frame(text = get("content", cleanMailCorpus))
tempDf$text <- as.character(mailsDf$text)

```


```{r}

tempDf <-tempDf %>% unnest_tokens(word, text)

```

```{r}
head(tempDf)

```


```{r}
# sort the data frame and convert word to a factor column
mailsPlot <- cleanMailsTokensTfIdf %>%
  arrange(desc(tf_idf)) %>%
  mutate(word = factor(word, levels = rev(unique(word))))
```

```{r}
head(mailsPlot)
```

```{r}

# graph the top 10 tokens for both ham and spam
mailsPlot %>%
  filter(type %in% c('ham', 'spam')) %>%
  mutate(type = factor(type, levels = c('ham', 'spam'),
                        labels = c("Ham mails", "Spam mails"))) %>%
  group_by(type) %>%
  top_n(10) %>%
  ungroup() %>%
  ggplot(aes(word, tf_idf)) +
  geom_col(fill = "steelblue") +
  labs(x = NULL, y = "tf-idf") +
  facet_wrap(~type, scales = "free") +
  coord_flip()

```

##### Apparently the visualisation for normal word count and tf-idf are different with words that appear more in the former almost not showing up in the later


##### Word cloud of top 50 ham and spam mails before and after tf-idf weighting. This is to see if the same words/terms that occured very frequently before weighting are still as such after weighting 


```{r}

# Before weighting
hams <- allMailsTokens %>% group_by(type) %>% filter(type == "ham") %>% count(word, sort = TRUE) %>% ungroup()
spams <- allMailsTokens %>% group_by(type) %>% filter(type == "spam") %>% count(word, sort = TRUE) %>% ungroup()


# After Weighting
hams50 <- top_n(subset(cleanMailsTokensTfIdf, type == 'ham'), 50)
spams50 <- top_n(subset(cleanMailsTokensTfIdf, type == 'spam'), 50)

```

```{r}

head(hams)

```

#### For Hams

```{r}

# Before weighting

wordcloud(words = hams$word, freq = hams$n, min.freq = 1,
          max.words=50, random.order=FALSE, rot.per=0.35, 
          colors=brewer.pal(8, "Dark2"))

```

```{r}

#After weighting
wordcloud(words = hams50$word, freq = hams50$n, min.freq = 1,
          max.words=50, random.order=FALSE, rot.per=0.35, 
          colors=brewer.pal(8, "Dark2"))

```

#### For Spams

```{r}
# Bfore weighting
wordcloud(words = spams$word, freq = hams$n, min.freq = 1,
          max.words=50, random.order=FALSE, rot.per=0.35, 
          colors=brewer.pal(8, "Dark2"))

```

```{r}

# After Weighting
wordcloud(words = spams50$word, freq = spams50$n, min.freq = 1,
          max.words=50, random.order=FALSE, rot.per=0.35, 
          colors=brewer.pal(8, "Dark2"))

```

##### Apparently the visualisations for normal word count and tf-idf are different with words that appear more in the former almost not showing up in the later


## Training, Classification and prediction
##### The next steps involve 1). Randomizing the emails order and quantifying each subset 2). Generating Training and test Document Term Matrices 3).Training & Test Label 4). Creating Proportions for training & test labels 5). Creating vector of most frequent occuring words 6). Filtering the Document Term Matrices 7). Creating training model from the training dataset 8). Making predictions on the test data set 8). Creating confusion matrix to give a summary statistics of the predictions

```{r}

set.seed(12)
randomisedSet <- tempDf[sample(nrow(tempDf)),]

```

```{r}

randomisedCorpus <- Corpus(VectorSource(randomisedSet$word))

```

```{r}
randommisedDTM <- DocumentTermMatrix(randomisedCorpus) 

```


```{r}
trainingDim <- dim(randomisedSet)[1]%/%4*3
trainingSet <- randomisedSet[1:trainingDim,]
txtSet <- dim(randomisedSet)[1]

```

```{r}
trainingSet <- randomisedSet[1:trainingDim,]
testingSet <- randomisedSet[(trainingDim+1):txtSet,]

```

```{r}

trainingDTM <- randommisedDTM[1:trainingDim,]
testDTM <- randommisedDTM[(trainingDim+1):txtSet,]

```


```{r}

trainingLabels <- trainingSet$type
testLabels <- testingSet$type

```

## Sparsity
##### I tried to use `trainingDTM` and `testDTM` in the training and testing but it almost sucked life out of my laptop so I want to reduce the model complexity by removing sparse terms from the model. That is, removing tokens which do not appear across many documents. It is similar to using tf-idf weighting, but directly deletes sparse variables from the document-term matrix. This results in a statistical learning model with a much smaller set of variables. The `tm` package contains the `removeSparseTerms()` function, which does this task. The first argument is a document-term matrix, and the second argument defines the maximal allowed sparsity in the range from 0 to 1. So for instance, sparse = .99 would remove any tokens which are missing from more than 95% of the documents in the corpus (i.e. the token must appear in at least 5% of the documents to be retained).

```{r}
threshold <- 0.5

minFreq = round(randommisedDTM$nrow*(threshold/100),0)

```

```{r}

frequentWords <- findFreqTerms(x = randommisedDTM, lowfreq = minFreq)

```

```{r}
length(testDTM)
length(frequentWords)
```

```{r}

trainingDTMFreq <- trainingDTM[ , frequentWords]
testDTMFreq <- testDTM[ , frequentWords]

dim(trainingDTMFreq)

```

```{r}
categoriseValues <- function(x) {
  x <- ifelse(x > 0, "Yes", "No")
}

```

```{r}

trainingText <- apply(trainingDTMFreq,  MARGIN =  2, categoriseValues)
testText <- apply(testDTMFreq, MARGIN = 2, categoriseValues)

```

## Naive Bayes Classifier
##### This is based on Bayes rule, fequency analysis of occurances of words and an independence assumption (the naive part). The Naive Bayes classifier assigns a probability that a new sample is in one class or another (spam or ham). Then, from the words contained or not contained in the message, it will compute the probability that a message is either a spam or ham. The Naive Bayes can be considered for this processes because the mails can be categorised as ham or spam (the input are categorical)
```{r}

classifier <- naiveBayes(trainingText, factor(trainingLabels))

```

```{r}
testPrediction <- predict(classifier, testText)

```

## Prediction Statistics

```{r}

confusionMatrix(data = testPrediction, reference = factor(testLabels),
                positive = "spam", dnn = c("Prediction", "Actual"))

```

## References
1. <a href="https://cfss.uchicago.edu/notes/supervised-text-classification/">Supervised Classification with Text Data</a>
2. <a href="https://www3.nd.edu/~steve/computing_with_data/20_text_mining/text_mining_example.html#/">Text mining example: spam filtering</a>


